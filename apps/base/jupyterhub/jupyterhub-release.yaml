apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: jupyterhub
  namespace: jupyterhub
spec:
  releaseName: jupyterhub
  targetNamespace: jupyterhub
  chart:
    spec:
      chart: jupyterhub
      version: "4.1.0"
      sourceRef:
        kind: HelmRepository
        name: jupyterhub
  interval: 5m0s
  values:
    hub:
      extraConfig:
        profiles: |
          exec(open('/usr/local/etc/jupyterhub/profiles.py').read())
        customTemplate: |
          import os
          c.JupyterHub.template_paths = ['/usr/local/share/jupyterhub/custom_templates']
        pre-spawn-hook: |
          # Load the pre-spawn hook from the ConfigMap
          import os
          import logging

          # Get logger
          log = logging.getLogger('jupyterhub.config')

          # Read the hook from the mounted ConfigMap
          hook_path = '/usr/local/etc/jupyterhub/mount-efs.py'

          if os.path.exists(hook_path):
              with open(hook_path, 'r') as f:
                  hook_code = f.read()

              # Execute the hook code to define the function
              exec(hook_code, globals())

              # Set the pre-spawn hook
              c.KubeSpawner.pre_spawn_hook = pre_spawn_hook
              log.info("Pre-spawn hook loaded successfully")
          else:
              log.warning(f"Pre-spawn hook file not found at {hook_path}")

      extraVolumes:
        - name: hub-profile-config
          configMap:
            name: hub-profile-config
        - name: custom-templates
          configMap:
            name: jupyterhub-custom-templates
        - name: pre-spawn-hook
          configMap:
            name: pre-spawn-hook

      extraVolumeMounts:
        - name: hub-profile-config
          mountPath: /usr/local/etc/jupyterhub/profiles.py
          subPath: profiles.py
          readOnly: true
        - name: custom-templates
          mountPath: /usr/local/share/jupyterhub/custom_templates
          readOnly: true
        - name: pre-spawn-hook
          mountPath: /usr/local/etc/jupyterhub/mount-efs.py
          subPath: mount-efs.py
          readOnly: true

    # This is the default configuration for single user pods
    singleuser:
      # Use the piksel-core image from ECR
      image:
        name: "686410905891.dkr.ecr.ap-southeast-3.amazonaws.com/piksel-core"
        tag: "jupyter-v20250808-132239"
        pullPolicy: "Always"

      # Start with JupyterLab
      defaultUrl: "/lab/tree/notebooks/README.md"

      nodeSelector:
        # Use the user-scheduler node selector
        karpenter.sh/capacity-type: on-demand

      #  Run bash script define in singleuser.yaml
      lifecycleHooks:
        postStart:
          exec:
            command:
              - "bash"
              - "/etc/singleuser/k8s-lifecycle-hook-post-start.sh"

      # Persistent storage configuration
      storage:
        type: dynamic
        capacity: 10Gi
        homeMountPath: /home/jovyan
        dynamic:
          storageClass: gp3
          pvcNameTemplate: claim-{username}
          volumeNameTemplate: volume-{username}
          storageAccessModes:
            - ReadWriteOnce

        # Mount the custom Configmap as a volume
        extraVolumes:
          - name: lifecycle-hook-script
            configMap:
              name: user-etc-singleuser
              defaultMode: 0755

        extraVolumeMounts:
          - name: lifecycle-hook-script
            mountPath: /etc/singleuser
            readOnly: true

      # Default user profile
      profileList:
        - display_name: "Standard Environment"
          description: "Jupyter environment with 2 CPUs and 12GB RAM"
          default: true # This makes it the default
          kubespawner_override:
            cpu_guarantee: 1
            cpu_limit: 2
            mem_guarantee: 12G
            mem_limit: 12G

      # Extra environment variables for single user pods
      extraEnv:
        # // Jupyter Environments
        # JUPYTER_TRUST_XHEADERS: "true"
        # // AWS Configuration
        AWS_NO_SIGN_REQUEST: "YES"
        # // GDAL / Rasterio environment variables
        GDAL_DISABLE_READDIR_ON_OPEN: "EMPTY_DIR"
        GDAL_HTTP_MAX_RETRY: "10"
        GDAL_HTTP_RETRY_DELAY: "1"
        GDAL_HTTP_MERGE_CONSECUTIVE_RANGES: "YES"
        # // Dask configuration
        DASK_LABEXTENSION__FACTORY__MODULE: "dask_gateway"
        DASK_LABEXTENSION__FACTORY__CLASS: "GatewayCluster"
        # DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: "{JUPYTER_IMAGE}"
        # DASK_DISTRIBUTED__DASHBOARD__LINK: "{JUPYTERHUB_SERVICE_PREFIX}proxy/{port}/status"
        # // OpenDataCube configuration
        ODC_DEFAULT_DB_HOSTNAME: "pg-proxy-service.database.svc.cluster.local"
        ODC_DEFAULT_DB_PORT: "6432"
        ODC_DEFAULT_DB_USERNAME: "odcread"
        ODC_DEFAULT_DB_DATABASE: "odc"
        ODC_DEFAULT_INDEX_DRIVER: "postgis"

      # set ServiceAccount to assume IAM role
      serviceAccountName: user-read

      # Additional Configs to be used in pair with the user-scheduler
      # extraPodConfig:
      #   topologySpreadConstraints:
      #     - maxSkew: 1 # Strict: Keep pod counts within 1 of each other across zones
      #       topologyKey: topology.kubernetes.io/zone
      #       whenUnsatisfiable: ScheduleAnyway
      #       labelSelector:
      #         matchLabels:
      #           component: singleuser-server # Matches single-user labels
      #     - maxSkew: 2
      #       topologyKey: kubernetes.io/hostname # Fallback: Spread across individual nodes to reduce hotspots
      #       whenUnsatisfiable: ScheduleAnyway
      #       labelSelector:
      #         matchLabels:
      #           component: singleuser-server

    # Scheduling relates to the user-scheduler pods and user-placeholder pods.
    scheduling:
      userScheduler:
        enabled: true
        replicas: 2
        logLevel: 4

        plugins:
          score:
            disabled:
              - name: NodeResourcesBalancedAllocation # Keep disabled: Avoids conflicts with custom fit strategy
              - name: NodeAffinity
              - name: InterPodAffinity
              - name: NodeResourcesFit
              - name: ImageLocality

            enabled: # Tweak weights: Lower NodeResourcesFit, boost ImageLocality and PodTopologySpread for speed/isolation
              - name: NodeAffinity
                weight: 14631 # Keep high: Ensures pods go to preferred nodes
              - name: InterPodAffinity
                weight: 1331 # Keep high: Helps group related pods, reducing network latency
              - name: NodeResourcesFit
                weight: 50 # Reduce from 121 to de-emphasize resource packing; pairs with LeastAllocated below for spreading
              - name: ImageLocality
                weight: 50 # Increase from 11 to prioritize nodes where images are pre-pulledâ€”speeds up startups
              - name: PodTopologySpread
                weight: 80 # Increase to ensure pods are spread across zones/nodes for better availability

        pluginConfig:
          - name: NodeResourcesFit
            args:
              scoringStrategy:
                type: LeastAllocated # Tweak: Switch from MostAllocated to LeastAllocated. Spreads pods across nodes, reducing contention and improving per-user performance/isolation. UX > cost here!
      podPriority:
        enabled: true
      userPlaceholder:
        enabled: false
        replicas: 3
        resources:
          requests:
            cpu: 2
            memory: 12Gi
          limits:
            cpu: 2
            memory: 12Gi

    # Proxy Configuration
    proxy:
      service:
        type: ClusterIP

    # Culling Configuration
    cull:
      enabled: true
      timeout: 3600 # 1 hour idle timeout
      every: 600 # Check every 10 minutes
