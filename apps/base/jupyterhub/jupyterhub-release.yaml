apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: jupyterhub
  namespace: jupyterhub
spec:
  releaseName: jupyterhub
  targetNamespace: jupyterhub
  chart:
    spec:
      chart: jupyterhub
      version: "4.1.0"
      sourceRef:
        kind: HelmRepository
        name: jupyterhub
  interval: 5m0s
  values:
    hub:
      extraConfig:
        profiles: |
          exec(open('/usr/local/etc/jupyterhub/profiles.py').read())
        customTemplate: |
          import os
          c.JupyterHub.template_paths = ['/usr/local/share/jupyterhub/custom_templates']
        pre-spawn-hook: |
          # Load the pre-spawn hook from the ConfigMap
          import os
          import logging

          # Get logger
          log = logging.getLogger('jupyterhub.config')

          # Read the hook from the mounted ConfigMap
          hook_path = '/usr/local/etc/jupyterhub/mount-efs.py'

          if os.path.exists(hook_path):
              with open(hook_path, 'r') as f:
                  hook_code = f.read()

              # Execute the hook code to define the function
              exec(hook_code, globals())

              # Set the pre-spawn hook
              c.KubeSpawner.pre_spawn_hook = pre_spawn_hook
              log.info("Pre-spawn hook loaded successfully")
          else:
              log.warning(f"Pre-spawn hook file not found at {hook_path}")

      extraVolumes:
        - name: hub-profile-config
          configMap:
            name: hub-profile-config
        - name: custom-templates
          configMap:
            name: jupyterhub-custom-templates
        - name: pre-spawn-hook
          configMap:
            name: pre-spawn-hook

      extraVolumeMounts:
        - name: hub-profile-config
          mountPath: /usr/local/etc/jupyterhub/profiles.py
          subPath: profiles.py
          readOnly: true
        - name: custom-templates
          mountPath: /usr/local/share/jupyterhub/custom_templates
          readOnly: true
        - name: pre-spawn-hook
          mountPath: /usr/local/etc/jupyterhub/mount-efs.py
          subPath: mount-efs.py
          readOnly: true

    # This is the default configuration for single user pods
    singleuser:
      # Use the piksel-core image from ECR
      image:
        name: "686410905891.dkr.ecr.ap-southeast-3.amazonaws.com/piksel-core"
        tag: "jupyter-v1.0"
        pullPolicy: "Always"

      # Start with JupyterLab
      defaultUrl: "/lab/tree/jovyan/notebooks/README.md"

      #  Run bash script define in singleuser.yaml
      lifecycleHooks:
        postStart:
          exec:
            command:
              - "bash"
              - "/etc/singleuser/k8s-lifecycle-hook-post-start.sh"

      # Persistent storage configuration
      storage:
        type: dynamic
        capacity: 10Gi
        homeMountPath: /home/jovyan
        dynamic:
          storageClass: gp3
          pvcNameTemplate: claim-{username}
          volumeNameTemplate: volume-{username}
          storageAccessModes:
            - ReadWriteOnce

        # Mount the custom Configmap as a volume
        extraVolumes:
          - name: lifecycle-hook-script
            configMap:
              name: user-etc-singleuser
              defaultMode: 0755

        extraVolumeMounts:
          - name: lifecycle-hook-script
            mountPath: /etc/singleuser
            readOnly: true

      # Default user profile
      profileList:
        - display_name: "Standard Environment"
          description: "Jupyter environment with 2 CPUs and 12GB RAM"
          default: true # This makes it the default
          kubespawner_override:
            cpu_guarantee: 1
            cpu_limit: 2
            mem_guarantee: 12G
            mem_limit: 12G

      # Extra environment variables for single user pods
      extraEnv:
        # // AWS Configuration
        AWS_NO_SIGN_REQUEST: "YES"
        # // GDAL / Rasterio environment variables
        GDAL_DISABLE_READDIR_ON_OPEN: "EMPTY_DIR"
        GDAL_HTTP_MAX_RETRY: "10"
        GDAL_HTTP_RETRY_DELAY: "1"
        GDAL_HTTP_MERGE_CONSECUTIVE_RANGES: "YES"
        # // Dask configuration
        DASK_LABEXTENSION__FACTORY__MODULE: "dask_gateway"
        DASK_LABEXTENSION__FACTORY__CLASS: "GatewayCluster"
        # DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: "{JUPYTER_IMAGE}"
        # DASK_DISTRIBUTED__DASHBOARD__LINK: "{JUPYTERHUB_SERVICE_PREFIX}proxy/{port}/status"
        # // OpenDataCube configuration
        ODC_DEFAULT_DB_HOSTNAME: "pg-proxy-service.database.svc.cluster.local"
        ODC_DEFAULT_DB_PORT: "6432"
        ODC_DEFAULT_DB_USERNAME: "odcread"
        ODC_DEFAULT_DB_DATABASE: "odc"
        ODC_DEFAULT_INDEX_DRIVER: "postgis"

      # set ServiceAccount to assume IAM role
      serviceAccountName: user-read

    # Proxy Configuration
    proxy:
      service:
        type: ClusterIP
      https:
        enabled: true
        type: offload # Let ingress handle SSL termination

    # Culling Configuration
    cull:
      enabled: true
      timeout: 3600 # 1 hour idle timeout
      every: 600 # Check every 10 minutes
