apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: datacube-bulk-indexer
  namespace: argo-workflows
spec:
  serviceAccountName: argo-workflows-executor
  entrypoint: grid-bulk-indexing
  parallelism: 10
  podGC:
    strategy: OnPodCompletion
  podMetadata:
    annotations:
      karpenter.sh/do-not-disrupt: "true"
    labels:
      app: datacube-bulk-indexer
  ttlStrategy:
    secondsAfterCompletion: 300
  podMetadata:
    annotations:
      karpenter.sh/do-not-disrupt: "true"
    labels:
      app: datacube-bulk-indexer
  arguments:
    parameters:
      - name: catalog
        value: "https://earth-search.aws.element84.com/v1/"
      - name: collections
        value: "sentinel-2-c1-l2a"
      - name: datetime
        value: "2020-04-01/2020-09-30"
      - name: arguments
        value: "--rename-product=s2_l2a"
      - name: time-chunks
        value: "3" # process 3 months at a time
      # Grid parameters
      - name: min-lon
        value: "114"
      - name: max-lon
        value: "116"
      - name: min-lat
        value: "-9"
      - name: max-lat
        value: "-7"
      - name: cell-size
        value: "1"
  templates:
    - name: grid-bulk-indexing
      steps:
        # Step 1: Generate time chunks
        - - name: generate-time-chunks
            template: gen-time-chunks-artifact
            arguments:
              parameters:
                - name: datetime
                  value: "{{workflow.parameters.datetime}}"
                - name: chunk-months
                  value: "{{workflow.parameters.time-chunks}}"

        # Step 2: Generate grid cells dynamically
        - - name: generate-grid
            template: gen-grid-cells-artifact
            arguments:
              parameters:
                - name: min-lon
                  value: "{{workflow.parameters.min-lon}}"
                - name: max-lon
                  value: "{{workflow.parameters.max-lon}}"
                - name: min-lat
                  value: "{{workflow.parameters.min-lat}}"
                - name: max-lat
                  value: "{{workflow.parameters.max-lat}}"
                - name: cell-size
                  value: "{{workflow.parameters.cell-size}}"

        # Step 3: Generate all individual tasks
        - - name: generate-all-tasks
            template: generate-all-tasks
            arguments:
              artifacts:
                - name: time-chunks-file
                  from: "{{steps.generate-time-chunks.outputs.artifacts.result}}"
                - name: grid-cells-file
                  from: "{{steps.generate-grid.outputs.artifacts.result}}"

        # Step 4: Process each task individually
        - - name: indexing-tasks
            template: indexing-tasks
            arguments:
              parameters:
                - name: catalog
                  value: "{{workflow.parameters.catalog}}"
                - name: collections
                  value: "{{workflow.parameters.collections}}"
                - name: arguments
                  value: "{{workflow.parameters.arguments}}"
                - name: datetime
                  value: "{{item.datetime}}"
                - name: bbox
                  value: "{{item.bbox}}"
                - name: grid-id
                  value: "{{item.grid_id}}"
            withParam: "{{steps.generate-all-tasks.outputs.parameters.all-tasks}}"

    # Generate time chunks
    - name: gen-time-chunks-artifact
      inputs:
        parameters:
          - name: datetime
          - name: chunk-months
      outputs:
        artifacts:
          - name: result
            path: /tmp/time-chunks.json
      script:
        image: python:3.9-alpine
        command: [python]
        source: |
          import subprocess
          import sys

          # Install dateutil
          subprocess.check_call([sys.executable, "-m", "pip", "install", "python-dateutil"])

          import json
          from datetime import datetime
          from dateutil.relativedelta import relativedelta

          datetime_param = "{{inputs.parameters.datetime}}"
          chunk_months = int("{{inputs.parameters.chunk-months}}")

          # Parse datetime parameter
          start_str, end_str = datetime_param.split('/')
          start_date = datetime.strptime(start_str, "%Y-%m-%d")
          end_date = datetime.strptime(end_str, "%Y-%m-%d")

          time_chunks = []
          current_date = start_date

          while current_date < end_date:
            # Add months using relativedelta
            chunk_end = current_date + relativedelta(months=chunk_months)

            # Don't exceed the overall end date
            if chunk_end > end_date:
                chunk_end = end_date

            period_str = f"{current_date.strftime('%Y-%m-%d')}/{chunk_end.strftime('%Y-%m-%d')}"
            time_id = f"{current_date.strftime('%Y%m')}_{chunk_end.strftime('%Y%m')}"

            time_chunks.append({
              "id": time_id,
              "datetime": period_str,
              "start": current_date.strftime('%Y-%m-%d'),
              "end": chunk_end.strftime('%Y-%m-%d')
            })

            current_date = chunk_end

          with open('/tmp/time-chunks.json', 'w') as f:
            json.dump(time_chunks, f)

          print(f"Generated {len(time_chunks)} time chunks", file=sys.stderr)

    # Generate grid cells dynamically
    - name: gen-grid-cells-artifact
      inputs:
        parameters:
          - name: min-lon
          - name: max-lon
          - name: min-lat
          - name: max-lat
          - name: cell-size
      outputs:
        artifacts:
          - name: result
            path: /tmp/grid-cells.json
      script:
        image: python:3.9-alpine
        command: [python]
        source: |
          import json
          import sys

          min_lon = float("{{inputs.parameters.min-lon}}")
          max_lon = float("{{inputs.parameters.max-lon}}")
          min_lat = float("{{inputs.parameters.min-lat}}")
          max_lat = float("{{inputs.parameters.max-lat}}")
          cell_size = float("{{inputs.parameters.cell-size}}")

          grid_cells = []

          # Generate grid cells
          lon = min_lon
          while lon < max_lon:
            lat = min_lat
            while lat < max_lat:
              # Create bbox: west,south,east,north
              bbox = f"{lon},{lat},{min(lon + cell_size, max_lon)},{min(lat + cell_size, max_lat)}"
              grid_id = f"grid_{lon}_{lat}".replace(".", "_").replace("-", "minus")

              grid_cells.append({
                "id": grid_id,
                "bbox": bbox,
              })

              lat += cell_size
            lon += cell_size

          with open('/tmp/grid-cells.json', 'w') as f:
            json.dump(grid_cells, f)

          print(f"Generated {len(grid_cells)} grid cells", file=sys.stderr)

    - name: generate-all-tasks
      inputs:
        artifacts:
          - name: time-chunks-file
            path: /tmp/time-chunks.json
          - name: grid-cells-file
            path: /tmp/grid-cells.json
      outputs:
        parameters:
          - name: all-tasks
            valueFrom:
              path: /tmp/all-tasks.json
      script:
        image: python:3.9-alpine
        command: [python]
        source: |
          import json
          import sys

          # Read inputs
          with open('/tmp/time-chunks.json', 'r') as f:
              time_chunks = json.load(f)
          with open('/tmp/grid-cells.json', 'r') as f:
              grid_cells = json.load(f)

          # Generate all individual tasks
          all_tasks = []
          for time_chunk in time_chunks:
              for grid_cell in grid_cells:
                  task = {
                      "datetime": time_chunk["datetime"],
                      "bbox": grid_cell["bbox"],
                      "grid_id": grid_cell["id"]
                  }
                  all_tasks.append(task)

          # Write as JSON
          with open('/tmp/all-tasks.json', 'w') as f:
              json.dump(all_tasks, f, ensure_ascii=False)

          # Also print to stdout for parameter capture
          print(json.dumps(all_tasks, ensure_ascii=False))

          print(f"Generated {len(all_tasks)} individual tasks", file=sys.stderr)

    # Main indexing task template
    - name: indexing-tasks
      inputs:
        parameters:
          - name: catalog
          - name: collections
          - name: arguments
          - name: datetime
          - name: bbox
          - name: grid-id
      container:
        name: odc-container
        image: 686410905891.dkr.ecr.ap-southeast-3.amazonaws.com/piksel-core:odc-v0.2.0
        command:
          - bash
        args:
          - "-c"
          - |
            echo "ðŸŒ Processing grid cell: {{inputs.parameters.grid-id}}"
            echo "ðŸ“… Datetime: {{inputs.parameters.datetime}}"
            echo "ðŸ“ Bbox: {{inputs.parameters.bbox}}"

            stac-to-dc \
            --catalog-href={{inputs.parameters.catalog}} \
            --collections={{inputs.parameters.collections}} \
            --bbox={{inputs.parameters.bbox}} \
            --datetime={{inputs.parameters.datetime}} \
            {{inputs.parameters.arguments}} \

            echo "Completed grid cell: {{inputs.parameters.grid-id}}"
        env:
          - name: ODC_DEFAULT_DB_HOSTNAME
            value: "pg-proxy-service.database.svc.cluster.local"
          - name: ODC_DEFAULT_DB_PORT
            value: "6432"
          - name: ODC_DEFAULT_DB_USERNAME
            valueFrom:
              secretKeyRef:
                name: odc
                key: username
          - name: ODC_DEFAULT_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: odc
                key: password
          - name: ODC_DEFAULT_DB_DATABASE
            valueFrom:
              secretKeyRef:
                name: odc
                key: username
          - name: ODC_DEFAULT_INDEX_DRIVER
            value: "postgis"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2"
