# Test Log 3: GPU Node Pool and Device Plugin Test Log

**Date:** July 07, 2025

### Test Overview

Tested the end-to-end provisioning and usage of GPU nodes using Karpenter, the NVIDIA device plugin, and a CUDA test pod. The configuration included:

- **Karpenter EC2NodeClass and NodePool** for GPU instances (g5.\* types), with appropriate labels, taints, and limits.
- **NVIDIA k8s-device-plugin** deployed via Helm (Flux) in version `0.14.3`.
- **Test pod** running `nvidia/cuda:12.9.1-base-ubuntu22.04`, which waits for `/dev/nvidia0`, runs `nvidia-smi`, and outputs GPU and memory status.

### Test Pod Logs

```
=== GPU Test Starting ===
Waiting for GPU device plugin...
=== GPU Device Found ===
Fri Jul  4 09:55:28 2025
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |
|  0%   30C    P8              9W /  300W |       0MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
=== GPU Memory Info ===
memory.total [MiB], memory.used [MiB], memory.free [MiB]
23028 MiB, 0 MiB, 22837 MiB
=== Keeping container alive for 10 minutes ===
```

### Test Results

- GPU node was successfully provisioned and detected by the test pod.
- The `nvidia-smi` output confirmed the presence of an NVIDIA A10G GPU with expected driver and CUDA versions.
- GPU memory was available and unused as expected for an idle pod.
- No running GPU processes were present, as expected for the test scenario.

**Conclusion:**
The current setup for GPU node provisioning and device plugin deployment is functional. The test pod was able to detect and interact with the GPU device as intended.

### Technical Note on NVIDIA Device Plugin Version

Attempts to upgrade the NVIDIA k8s-device-plugin to version `0.17.x` were unsuccessful. The newer version introduces significant breaking changes, including:

- Support for only Kubernetes 1.27 and later.
- Changes in resource naming and configuration (e.g., `nvidia.com/gpu` vs `nvidia.com/gpu.shared`).
- Modifications to Helm values and plugin behavior (such as MIG support and node selector handling).

**Recommendation:**
Remain on version `0.14.3` for the NVIDIA device plugin until the migration path is fully reviewed and tested in a non-production environment. Before attempting an upgrade to `0.17.x` or newer, consult the [official repository](https://github.com/NVIDIA/k8s-device-plugin/blob/main) and review all Helm values and resource requests for compatibility.
